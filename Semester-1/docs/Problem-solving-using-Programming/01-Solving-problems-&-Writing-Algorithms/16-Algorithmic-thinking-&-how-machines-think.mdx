# Algorithmic Thinking and How Machines Think

import Youtube from "@site/src/components/Youtube";

## Lesson Objective
In this lesson, you will understand what thinking algorithmically actually means when you’re dealing with computers and how exactly we can think in terms of algorithms to make computing hardware perform the tasks we require. This lesson might answer your questions of why intelligent engineers are needed for software engineering. Happy Learning!

## Why is it important to understand how computers think(or do they🤔)?

The reason it is important to understand how computers actually “think” (i.e. perform calculations that make them capable of even defeating the world champions at chess) is because you can make the computers perform various tasks only when you know how the computers work on these tasks at a lower level. You will be able to create better computer programs if you understand how computers actually interpret the instructions specified in these computer programs.

## Introduction to the Lesson
In this lesson we are going to discover whether computers have “thinking” power or not and how a software programmer can make computers perform tasks that make them seem like they are capable of thinking. 
We will also understand what algorithms are and why thinking in terms of algorithms is important to make computers perform the task that you want them to perform. 

**We will discuss the following major points in this lesson**
 - How did computers become so clever?

 - Definitions and examples of algorithms

 - How to think algorithmically as a Programmer? 

 - Flowcharts & Pseudocode


### How did computers become so clever?

Before discussing how computers are able to do such miraculous things like beating the world chess champion or even having a conversation with a human, we have to discuss whether computers can think on their own or not.
:thought_balloon: Can computers actually think ? 

>Thinking in the human sense involves imagination, gathering all the information you have, analysing the actual situation, comparing it over and over again, in less than a second and then making a choice with your own will and these choices might not even be based on logic or rules.

>*Computers simply cannot think*. If you thought that computers are able to think by looking at various applications like voice recognition and face recognition, you have been deceived by the complexity of these applications. But, at a deeper level, these applications also contain only basic programming instructions and only follow the instructions to perform various tasks.

**How do computers seem to be able to think?**

How do these devices always do PRECISELY what you tell them? A computer program is essentially a list of very specific rules and instructions - perhaps like a cooking recipe. It follows that recipe to the letter - never straying from it - never having an original thought of any kind.

The actual instructions for the computer have to be written very precisely in special languages (called **programming languages** which you will learn more about in upcoming modules).

With that said, we are now in the age of “Artificial Intelligence” - where we write a set of rules for the mindless machine that make it seem like it’s thinking. Let’s look at a program to play **Tic-Tac-Toe**. 


**Artificial Intelligence**
Let’s say we write a program to play *Tic-Tac-Toe*. AI programs create relations between decisions and results, if it loses (which it will do a lot initially) - it will never play that same way again. So eventually, after enough time playing real human players, it’ll learn a set of moves that will allow it to win (or at least draw) every single time. You will learn more about AI in more advanced courses in the future.


**The Recipe Analogy**

![](https://s3.ap-south-1.amazonaws.com/kalvi-education.github.io/problem-solving/recipe_analogy.jpeg)

Let us continue with the cooking analogy and consider an imaginary person who has never been to a kitchen nor have they seen anyone cook anything before i.e. . If you require this person to cook a dish for you, what would you do?

You would have to give them a recipe which would involve a step-by-step detailed guide along with all considerations like if an ingredient X is not available, then what to do and so on. 

>Computers also work the same way, we give them a step-by-step guide with all possible considerations so that they can perform the desired task because as we have come to realize, computers are **dumb machines that do exactly as they are instructed to** (no more and no less). They are basically a group of circuits with 0s and 1s(representation for low voltage and high voltage) flowing in them. Thus, we need to provide step-by-step and clear instructions to the computer. These **step-by-step instructions** are what we call **algorithms**, which we’ll discuss in detail shortly. 


Let's get familiar with the term *"Alogrithmic Thinking".*
*(Duration : till 1:14)*

<Youtube videoId="uvsQPXJaulU" start="00" end="74"/>

## Definition of Algorithmic Thinking

 - Algorithmic thinking is a systematic way of thinking through problems and solutions in a way that’s similar to how a computer would run. This approach automates the problem-solving process by creating a series of systematic, logical steps for the computer(or even a human) to follow.

 - Algorithmic thinking is not solving for a specific answer; instead, it solves how to build a sequential, complete, and replicable process that has an end point. 

 - Algorithmic thinking is a component of computational thinking about which you have already studied

**What are Algorithms?**

 - Algorithms are a series of instructions that are followed, step by step, to do something useful or solve a problem. For example, you could consider a cake recipe an algorithm for making a cake.

 - In computer science, algorithms provide computers with a successive guide to completing actions. They consist of a precise list of instructions that outline exactly how to complete a task.
 
 - In higher level applications, algorithms act in complex patterns, each using smaller and smaller sub-methods which are built up to the program as a whole.

Let's get familiar with the term *"Alogrithms".*
*(Duration : from 1:14 till 3:45)*

<Youtube videoId="uvsQPXJaulU" start="74" end="225"/>



**Essential Properties of Algorithms**

`Input` An algorithm should have 0 or more well-defined inputs.

`Output` An algorithm should have 1 or more well-defined outputs, and should match the desired output.

`Definiteness` The sequence of events and the details of each step must be clearly defined along with the mechanism to handle errors

`Unambiguity` Algorithms should be clear and unambiguous. Each of its steps (or phases), and their inputs/outputs should be clear and must lead to only one meaning.

`Effectiveness` The operations must be doable and the algorithm must be feasible in terms of computing resources.

`Finiteness` Algorithms must terminate after a finite number of steps


**Notations to represent algorithms**

1. Natural Languages : Languages used by humans like English can be used to write out algorithms just like a cooking recipe is written out.

2. Flowchart : A diagramatic representation

3. Pseudo Code : A fake code

We will learn about flowcharts and pseudocodes in detail shortly.

![](https://media0.giphy.com/media/5nSIhPtqyXqEjnjNGJ/giphy.gif?cid=ecf05e47lxg2vhhd1qk6tjc52q7uphc04i78r8f59lfsm1i8&rid=giphy.gif&ct=g)

*Source: [Giphy](https://giphy.com/gifs/stay-tuned-abo-aboshop-5nSIhPtqyXqEjnjNGJ)*


## Flowcharts
It is a **diagrammatic notation** that uses various graphical symbols to represent an algorithm and it produces the most unambiguous and easy to understand representation of an algorithm.

It makes use of symbols which are connected among them to indicate the flow of information and processing. 

>An example for a flowchart considering a real life situation is described as below









